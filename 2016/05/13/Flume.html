<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>Flume 学习笔记</title>
    <meta name="description" content="" />

    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="shortcut icon" href="//gongxiancao.github.io/themes/Casper/favicon.ico">

    <link rel="stylesheet" type="text/css" href="//gongxiancao.github.io/themes/Casper/assets/css/screen.css?v=1.0.0" />
    <link rel="stylesheet" type="text/css" href="//fonts.googleapis.com/css?family=Merriweather:300,700,700italic,300italic|Open+Sans:700,400" />

    <link rel="canonical" href="https://gongxiancao.github.io/2016/05/13/Flume.html" />
    
    <meta property="og:site_name" content="" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Flume 学习笔记" />
    <meta property="og:description" content="引子 （第一篇github博客成功发表，感谢hubpress！） 缘起：虽然地铁上总是大部分时间都用来看头条，玩游戏，但是微信上的架构师订阅号总能偶尔让人眼前一亮，什么Flume，Kafka，Thrift，ZooKeeper时不时敲打着我的羞耻心、上进心和求知欲。作为我这个级别的人，是不是得看看，混个脸熟？……但也仅仅是脸熟而已。 直到最近，发现很难躲过去，项目需要处理大量的日志数据，发现几点目前采用的MongoDB MapReduce方案的问题： 虽然MongoDB的MapReduce虽然能够利用分片位置提高速度，但是需要数据库进程级别的锁，难以在不..." />
    <meta property="og:url" content="https://gongxiancao.github.io/2016/05/13/Flume.html" />
    <meta property="article:published_time" content="2016-05-12T16:00:00.000Z" />
    <meta property="article:modified_time" content="2016-05-17T04:44:14.058Z" />
    
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Flume 学习笔记" />
    <meta name="twitter:description" content="引子 （第一篇github博客成功发表，感谢hubpress！） 缘起：虽然地铁上总是大部分时间都用来看头条，玩游戏，但是微信上的架构师订阅号总能偶尔让人眼前一亮，什么Flume，Kafka，Thrift，ZooKeeper时不时敲打着我的羞耻心、上进心和求知欲。作为我这个级别的人，是不是得看看，混个脸熟？……但也仅仅是脸熟而已。 直到最近，发现很难躲过去，项目需要处理大量的日志数据，发现几点目前采用的MongoDB MapReduce方案的问题： 虽然MongoDB的MapReduce虽然能够利用分片位置提高速度，但是需要数据库进程级别的锁，难以在不..." />
    <meta name="twitter:url" content="https://gongxiancao.github.io/2016/05/13/Flume.html" />
    
    <script type="application/ld+json">
{
    "@context": "http://schema.org",
    "@type": "Article",
    "publisher": "",
    "author": {
        "@type": "Person",
        "name": null,
        "image": "https://avatars.githubusercontent.com/u/1942039?v=3",
        "url": "undefined/author/undefined",
        "sameAs": null
    },
    "headline": "Flume 学习笔记",
    "url": "https://gongxiancao.github.io/2016/05/13/Flume.html",
    "datePublished": "2016-05-12T16:00:00.000Z",
    "dateModified": "2016-05-17T04:44:14.058Z",
    "description": "引子 （第一篇github博客成功发表，感谢hubpress！） 缘起：虽然地铁上总是大部分时间都用来看头条，玩游戏，但是微信上的架构师订阅号总能偶尔让人眼前一亮，什么Flume，Kafka，Thrift，ZooKeeper时不时敲打着我的羞耻心、上进心和求知欲。作为我这个级别的人，是不是得看看，混个脸熟？……但也仅仅是脸熟而已。 直到最近，发现很难躲过去，项目需要处理大量的日志数据，发现几点目前采用的MongoDB MapReduce方案的问题： 虽然MongoDB的MapReduce虽然能够利用分片位置提高速度，但是需要数据库进程级别的锁，难以在不..."
}
    </script>

    <meta name="generator" content="Ghost ?" />
    <link rel="alternate" type="application/rss+xml" title="" href="https://gongxiancao.github.io/rss" />
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/styles/default.min.css">
</head>
<body class="post-template">

    


<header class="main-header post-head no-cover">
    <nav class="main-nav  clearfix">
        <a class="back-button icon-arrow-left" href="https://gongxiancao.github.io">Home</a>
        <!-- <a class="subscribe-button icon-feed" href="https://gongxiancao.github.io/rss/">Subscribe</a> -->
    </nav>
</header>

<main class="content" role="main">

    <article class="post">

        <header class="post-header">
            <h1 class="post-title">Flume 学习笔记</h1>
            <section class="post-meta">
                <time class="post-date" datetime="2016-05-13">13 May 2016</time> 
            </section>
        </header>

        <section class="post-content">
            <div class="sect1">
<h2 id="__">引子</h2>
<div class="sectionbody">
<div class="paragraph">
<p>（第一篇github博客成功发表，感谢hubpress！）</p>
</div>
<div class="paragraph">
<p>缘起：虽然地铁上总是大部分时间都用来看头条，玩游戏，但是微信上的架构师订阅号总能偶尔让人眼前一亮，什么Flume，Kafka，Thrift，ZooKeeper时不时敲打着我的羞耻心、上进心和求知欲。作为我这个级别的人，是不是得看看，混个脸熟？……但也仅仅是脸熟而已。</p>
</div>
<div class="paragraph">
<p>直到最近，发现很难躲过去，项目需要处理大量的日志数据，发现几点目前采用的MongoDB MapReduce方案的问题：
虽然MongoDB的MapReduce虽然能够利用分片位置提高速度，但是需要数据库进程级别的锁，难以在不影响日志写入的情况下，对数据进行分析处理，所以需要拆分。</p>
</div>
<div class="paragraph">
<p>但MongoDB的MapReduce要求源数据和输出数据在同一个数据库，要解决数据分析对产品环境的影响，就要求我们数据再存一份数据作为源给MapReduce用。一开始想到的是用副本集解决多份数据的问题，用Primary来接受写入的数据，Secondary来做MapReduce的数据源，但是由于MongoDB MapReduce要求源数据和输出数据在同一个数据库，“同一个数据库”，意味着要往Secondary写数据，这是MongoDB不允许的。难道要自己写个机制把数据从日志表同步到MapReduce的数据库吗？</p>
</div>
<div class="paragraph">
<p>考虑到MongoDB的写入速度，恐怕用来存储日志不太适合，直到看过更多前辈的文章，发现一般的方案是用Flume。</p>
</div>
<div class="paragraph">
<p>Flume的作用是收集日志，有各种好处，高可用，可扩展，可靠，高性能，分布式，重要的是，数据写到HDFS，数据分析有着落了。</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="___2">正文</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="___3">*安装</h3>
<div class="sect3">
<h4 id="__java">安装Java</h4>
<div class="paragraph">
<p><a href="https://www.digitalocean.com/community/tutorials/how-to-install-java-on-ubuntu-with-apt-get" class="bare">https://www.digitalocean.com/community/tutorials/how-to-install-java-on-ubuntu-with-apt-get</a></p>
</div>
</div>
<div class="sect3">
<h4 id="__hadoop">安装hadoop</h4>
<div class="paragraph">
<p><a href="http://www.bogotobogo.com/Hadoop/BigData_hadoop_Install_on_ubuntu_single_node_cluster.php" class="bare">http://www.bogotobogo.com/Hadoop/BigData_hadoop_Install_on_ubuntu_single_node_cluster.php</a></p>
</div>
</div>
<div class="sect3">
<h4 id="__flume">安装Flume</h4>
<div class="paragraph">
<p>先是Flume的安装，可以参考
<a href="http://hadooptutorial.info/apache-flume-installation/" class="bare">http://hadooptutorial.info/apache-flume-installation/</a></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>nu@node:~$ wget http://mirror.bit.edu.cn/apache/flume/1.6.0/apache-flume-1.6.0-bin.tar.gz
nu@node:~$ sudo mkdir /usr/lib/flume
nu@node:~$ sudo chmod -R 777 /usr/lib/flume
nu@node:~$ cp apache-flume-1.6.0-bin.tar.gz /usr/lib/flume/
nu@node:~$ cd /usr/lib/flume/
nu@node:/usr/lib/flume$ tar -xzf apache-flume-1.6.0-bin.tar.gz
nu@node:/usr/lib/flume$ ls
apache-flume-1.6.0-bin  apache-flume-1.6.0-bin.tar.gz
nu@node:/usr/lib/flume$</code></pre>
</div>
</div>
<div class="paragraph">
<p>添加到~/.bashrc</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>### Flume Home Directory
export FLUME_HOME="/usr/lib/flume/apache-flume-1.6.0-bin"
export FLUME_CONF_DIR="$FLUME_HOME/conf"
export FLUME_CLASSPATH="$FLUME_CONF_DIR"
export PATH="$FLUME_HOME/bin:$PATH"
### Flume Home Directory end</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>nu@node:/usr/lib/flume/apache-flume-1.6.0-bin/conf$ cp flume-env.sh.template flume-env.sh
nu@node:/usr/lib/flume/apache-flume-1.6.0-bin/conf$ vi flume-env.sh</code></pre>
</div>
</div>
<div class="paragraph">
<p>修改flume-env.sh添加：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code># export JAVA_HOME=/usr/lib/jvm/java-6-sun
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64</code></pre>
</div>
</div>
<div class="paragraph">
<p>运行flume-ng测试：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>nu@node:~$ flume-ng --help</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="___4">*简单示例</h3>
<div class="paragraph">
<p>点击 <a href="https://flume.apache.org/FlumeUserGuide.html" class="bare">https://flume.apache.org/FlumeUserGuide.html</a>
进行更深入的了解。</p>
</div>
<div class="sect4">
<h5 id="__1">示例1</h5>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>nu@node:/usr/lib/flume/apache-flume-1.6.0-bin/conf$ vi example.conf</code></pre>
</div>
</div>
<div class="paragraph">
<p>把下面的内容添加进去：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code># example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>nu@node:~$ cd /usr/lib/flume/apache-flume-1.6.0-bin
nu@node:/usr/lib/flume/apache-flume-1.6.0-bin$ flume-ng agent --conf conf --conf-file conf/example.conf --name a1 -Dflume.root.logger=INFO,console</code></pre>
</div>
</div>
<div class="paragraph">
<p>这里，如果你是按照 <a href="https://flume.apache.org/FlumeUserGuide.html" class="bare">https://flume.apache.org/FlumeUserGuide.html</a> 的指导，用下面的命令：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>nu@node:/usr/lib/flume/apache-flume-1.6.0-bin$ bin/flume-ng agent --conf conf --conf-file example.conf --name a1 -Dflume.root.logger=INFO,console</code></pre>
</div>
</div>
<div class="paragraph">
<p>如果你在flume conf目录下运行这个命令，他会提示错误：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>Warning: JAVA_HOME is not set!
Info: Including Hive libraries found via () for Hive access
+ exec /usr/bin/java -Xmx20m -Dflume.root.logger=INFO,console -cp 'conf:/usr/lib/flume/apache-flume-1.6.0-bin/lib/*:/lib/*' -Djava.library.path= org.apache.flume.node.Application --conf-file example.conf --name a1
log4j:WARN No appenders could be found for logger (org.apache.flume.lifecycle.LifecycleSupervisor).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</code></pre>
</div>
</div>
<div class="paragraph">
<p>即使你把JAVA_HOME设置好，log4j的错误依然还在。后来发现 --conf conf是指定配置文件的目录，所以要让上面的命令运行起来，需要在conf的上一层目录运行命令，并且example.conf要在那个目录下，或者也在conf下，而用下面的命令：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>bin/flume-ng agent --conf conf --conf-file conf/example.conf --name a1 -Dflume.root.logger=INFO,console</code></pre>
</div>
</div>
<div class="paragraph">
<p>现在agent已经启动，再起一个terminal窗口，执行：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>nu@node:~$ telnet localhost 44444
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.</code></pre>
</div>
</div>
<div class="paragraph">
<p>输入“Hello world!”，回车：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>Hello world!
OK</code></pre>
</div>
</div>
<div class="paragraph">
<p>在agent执行窗口会看到：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>2016-05-13 14:12:11,592 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:94)] Event: { headers:{} body: 48 65 6C 6C 6F 20 77 6F 72 6C 64 21 0D          Hello world!. }</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="___5">*第三方插件</h3>
<div class="paragraph">
<p>插件目录可以加到flume-env.sh中的FLUME_CLASSPATH，或者直接把插件装到plugins.d目录下面。
每个插件目录可以有三个子目录: lib, libext, native</p>
</div>
<div class="paragraph">
<p><strong>Avro RPC</strong></p>
</div>
<div class="paragraph">
<p>用下面命令发送日志文件到Flume：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>$ bin/flume-ng avro-client -H localhost -p 41414 -c conf -F /usr/logs/log.10</code></pre>
</div>
</div>
<div class="paragraph">
<p>插件比较多，就不一个一个学了，主要看看如何把数据存入hdfs。</p>
</div>
</div>
<div class="sect2">
<h3 id="__multi_agent_flow">*Multi-agent flow</h3>
<div class="paragraph">
<p><strong>Consolidation （合并）</strong></p>
</div>
<div class="paragraph">
<p><strong>Multiplexing （多工？）</strong></p>
</div>
</div>
<div class="sect2">
<h3 id="__hdfs">*把数据存入hdfs</h3>
<div class="paragraph">
<p>这是首要目标！</p>
</div>
<div class="paragraph">
<p>推荐一篇博客：http://scu.qfboys.com/blog/storage/flume-hdfs.html</p>
</div>
<div class="paragraph">
<p>配置两个agent：
netcat_avro.conf</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code># list the sources, sinks and channels for the agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# set channel for source
a1.sources.r1.channels = c1

# set channel for sink
a1.sinks.k1.channel = c1

# properties of r1
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# properties of c1
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# properties of k1
a1.sinks.k1.type = avro
a1.sinks.k1.hostname = localhost
a1.sinks.k1.port = 10001</code></pre>
</div>
</div>
<div class="paragraph">
<p>avro_hdfs.conf</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code># list the sources, sinks and channels for the agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# set channel for source
a1.sources.r1.channels = c1

# set channel for sink
a1.sinks.k1.channel = c1

# properties of r1
a1.sources.r1.type = avro
a1.sources.r1.bind = localhost
a1.sources.r1.port = 10001

# properties of c1
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# properties of k1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://localhost/flume/webdata</code></pre>
</div>
</div>
<div class="paragraph">
<p>运行avro_hdfs:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>nu@node:/usr/lib/flume/apache-flume-1.6.0-bin$ flume-ng agent --conf conf --conf-file conf/avro_hdfs.conf --name a1 -Dflume.root.logger=INFO,console</code></pre>
</div>
</div>
<div class="paragraph">
<p>看到这个错误：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>2016-05-13 23:18:04,801 (conf-file-poller-0) [ERROR - org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:145)] Failed to start agent because dependencies were not found in classpath. Error follows.
java.lang.NoClassDefFoundError: org/apache/hadoop/io/SequenceFile$CompressionType
	at org.apache.flume.sink.hdfs.HDFSEventSink.configure(HDFSEventSink.java:239)
	at org.apache.flume.conf.Configurables.configure(Configurables.java:41)
	at org.apache.flume.node.AbstractConfigurationProvider.loadSinks(AbstractConfigurationProvider.java:413)
	at org.apache.flume.node.AbstractConfigurationProvider.getConfiguration(AbstractConfigurationProvider.java:98)
	at org.apache.flume.node.PollingPropertiesFileConfigurationProvider$FileWatcherRunnable.run(PollingPropertiesFileConfigurationProvider.java:140)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.io.SequenceFile$CompressionType
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
	... 13 more</code></pre>
</div>
</div>
<div class="paragraph">
<p>谷歌到 <a href="http://scu.qfboys.com/blog/storage/flume-hdfs.html" class="bare">http://scu.qfboys.com/blog/storage/flume-hdfs.html</a> ，把各个jar考到flume/lib或加到FLUME_CLASSPATH里。注意由于hadoop版本不同，里面列的几个文件，你的版本里可能不是都有，根据情况选择对应的新版替换掉。对应于我的环境，FLUME_CLASSPATH的设置是：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>FLUME_CLASSPATH="/usr/local/hadoop/share/hadoop/common/hadoop-common-2.6.0.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.6.0.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.0.4.jar"</code></pre>
</div>
</div>
<div class="paragraph">
<p>要找到你的对应文件的位置，使用find：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>nu@node:/usr/lib/flume/apache-flume-1.6.0-bin$ find  /usr/local/hadoop/ -name commons-codec*
/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar
/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar
/usr/local/hadoop/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/commons-codec-1.4.jar
/usr/local/hadoop/share/hadoop/tools/lib/commons-codec-1.4.jar
/usr/local/hadoop/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/commons-codec-1.4.jar
/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar</code></pre>
</div>
</div>
<div class="paragraph">
<p>选择一个看起来合适的。</p>
</div>
<div class="paragraph">
<p>然后，再次运行</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>nu@node:/usr/lib/flume/apache-flume-1.6.0-bin$ flume-ng agent --conf conf --conf-file conf/avro_hdfs.conf --name a1 -Dflume.root.logger=INFO,console</code></pre>
</div>
</div>
<div class="paragraph">
<p>再起一个terminal窗口，运行：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>nu@node:/usr/lib/flume/apache-flume-1.6.0-bin$ flume-ng agent --conf conf --conf-file conf/netcat_avro.conf --name a1 -Dflume.root.logger=INFO,console</code></pre>
</div>
</div>
<div class="paragraph">
<p>然后，再起一个窗口，运行：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>nu@node:~$ telnet localhost 44444
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
Hellow world!
OK</code></pre>
</div>
</div>
<div class="paragraph">
<p>这个时候发现 avro_hdfs agent窗口报错：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>2016-05-14 00:57:48,148 (SinkRunner-PollingRunner-DefaultSinkProcessor) [WARN - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:455)] HDFS IO error
java.net.ConnectException: Call From node/127.0.1.1 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
	at sun.reflect.GeneratedConstructorAccessor6.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at $Proxy13.create(Unknown Source)</code></pre>
</div>
</div>
<div class="paragraph">
<p>这个错是因为当我根据 <a href="http://www.bogotobogo.com/Hadoop/BigData_hadoop_Install_on_ubuntu_single_node_cluster.php" class="bare">http://www.bogotobogo.com/Hadoop/BigData_hadoop_Install_on_ubuntu_single_node_cluster.php</a> 安装hadoop的时候，配了端口。我们看看现在那个端口是多少，打开
/usr/local/hadoop/etc/hadoop/core-site.xml</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;property&gt;
  &lt;name&gt;fs.default.name&lt;/name&gt;
  &lt;value&gt;hdfs://localhost:54310&lt;/value&gt;
  &lt;description&gt;The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.&lt;/description&gt;
 &lt;/property&gt;
&lt;/configuration&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>端口是54310，改一下avro_hdfs.conf:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>a1.sinks.k1.hdfs.path = hdfs://localhost:54310/flume/webdata</code></pre>
</div>
</div>
<div class="paragraph">
<p>再启动 avro_hdfs agent, 按上面的步骤再测试一遍，仍然报错：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>2016-05-14 23:30:22,559 (SinkRunner-PollingRunner-DefaultSinkProcessor) [WARN - org.apache.flume.sink.hdfs.HDFSEventSink.process(HDFSEventSink.java:455)] HDFS IO error
org.apache.hadoop.security.AccessControlException: Permission denied: user=nu, access=WRITE, inode="/flume/webdata":hduser:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkFsPermission(FSPermissionChecker.java:271)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:257)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:238)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:179)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6512)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6494)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6446)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2712)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2632)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2519)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:566)</code></pre>
</div>
</div>
<div class="paragraph">
<p>这个是因为，hadoop运行在hduser用户下，当前帐户nu，没有权限，可以通过下面方式来验证：</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>nu@node:/usr/lib/flume/apache-flume-1.6.0-bin$ sudo su hduser
hduser@node:/usr/lib/flume/apache-flume-1.6.0-bin$ source /home/nu/.bashrc
hduser@node:/usr/lib/flume/apache-flume-1.6.0-bin$ flume-ng agent --conf conf --conf-file conf/avro_hdfs.conf --name a1 -Dflume.root.logger=INFO,console</code></pre>
</div>
</div>
<div class="paragraph">
<p>再次测试，打开 <a href="http://node.guest:50070/explorer.html#/flume/webdata" class="bare">http://node.guest:50070/explorer.html#/flume/webdata</a> 页面，发现梦寐以求的文件：</p>
</div>
<div class="imageblock">
<div class="content">
<img src="/images/flume_hadoop_files.jpg" alt="flume hadoop files.jpg">
</div>
</div>
</div>
<div class="sect2">
<h3 id="___6">总结</h3>
<div class="paragraph">
<p>MongoDB的MapReduce的局限，很难直接拿来实现大数据收集和分析。因此Flume的需求比较强烈和迫切。在上面的场景中使用Flume需要安装Java，Hadoop，Flume，然后实作了 netcat&#8658;logger, netcat&#8658;avro&#8658;hdfs的agent flow。</p>
</div>
</div>
<div class="sect2">
<h3 id="___7">问题</h3>
<div class="paragraph">
<p>最后的截屏发现一个问题，那些文件的Block Size 128MB，而文件本身只有100～200B，看起来非常浪费。hdfs大文件偏好的设计，Flume就没有考虑吗？希望后面有时间把这个问题搞清楚。</p>
</div>
</div>
</div>
</div>
        </section>

        <footer class="post-footer">


            <figure class="author-image">
                <a class="img" href="" style="background-image: url(https://avatars.githubusercontent.com/u/1942039?v=3)"><span class="hidden">'s Picture</span></a>
            </figure>

            <section class="author">
                <h4><a href=""></a></h4>

                    <p>Read <a href="">more posts</a> by this author.</p>
                <div class="author-meta">
                    
                    
                </div>
            </section>


            <section class="share">
                <h4>Share this post</h4>
                <a class="icon-twitter" href="https://twitter.com/share?text=Flume%20%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0&amp;url=https://gongxiancao.github.io/2016/05/13/Flume.html"
                    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
                    <span class="hidden">Twitter</span>
                </a>
                <a class="icon-facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://gongxiancao.github.io/2016/05/13/Flume.html"
                    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
                    <span class="hidden">Facebook</span>
                </a>
                <a class="icon-google-plus" href="https://plus.google.com/share?url=https://gongxiancao.github.io/2016/05/13/Flume.html"
                   onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
                    <span class="hidden">Google+</span>
                </a>
            </section>

        </footer>


    </article>

</main>



    <footer class="site-footer clearfix">
        <section class="copyright"><a href="https://gongxiancao.github.io"></a> &copy; 2016</section>
        <section class="poweredby">Proudly published with <a href="http://hubpress.io">HubPress</a></section>
    </footer>

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/moment.js/2.9.0/moment-with-locales.min.js?v="></script> <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/8.4/highlight.min.js?v="></script> 
      <script type="text/javascript">
        jQuery( document ).ready(function() {
          // change date with ago
          jQuery('ago.ago').each(function(){
            var element = jQuery(this).parent();
            element.html( moment(element.text()).fromNow());
          });
        });

        hljs.initHighlightingOnLoad();      
      </script>

    <script type="text/javascript" src="//gongxiancao.github.io/themes/Casper/assets/js/jquery.fitvids.js?v=1.0.0"></script>
    <script type="text/javascript" src="//gongxiancao.github.io/themes/Casper/assets/js/index.js?v=1.0.0"></script>

</body>
</html>
